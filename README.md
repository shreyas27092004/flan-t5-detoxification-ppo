# Fine-Tuning FLAN-T5 for Detoxification with PPO and PEFT

This repository contains a Jupyter Notebook demonstrating how to fine-tune a `google/flan-t5-base` model to produce less toxic summaries. The project leverages Reinforcement Learning (RL) with Proximal Policy Optimization (PPO) and Parameter-Efficient Fine-Tuning (PEFT) to align the model's output with a desired behavior (non-toxicity).

## Overview

The core idea is to use a reward model to guide the fine-tuning process. Instead of providing explicit examples of "good" summaries, we reward the language model for generating outputs that a separate hate speech detection model deems non-toxic. This technique, a form of Reinforcement Learning from AI Feedback (RLAIF), allows for targeted adjustments to the model's behavior without extensive manual data labeling.

This project uses the Transformer Reinforcement Learning (`trl`) library to manage the PPO training loop, making the complex process of RL fine-tuning more accessible.

## Key Features

-   **Reinforcement Learning**: Utilizes Proximal Policy Optimization (PPO) to steer the model towards generating less toxic content.
-   **PEFT Integration**: Employs LoRA (Low-Rank Adaptation) for memory-efficient fine-tuning, updating only a small fraction of the model's parameters.
-   **Reward Modeling**: Uses Meta AI's `roberta-hate-speech` model as a reward function to score the toxicity of generated summaries.
-   **Quantitative Evaluation**: Measures the model's toxicity score before and after fine-tuning to assess the impact of the detoxification process.
-   **Qualitative Analysis**: Provides a side-by-side comparison of summaries generated by the original and the fine-tuned models.

## Tech Stack & Models

-   **Base Model**: `google/flan-t5-base` (instruction-tuned for summarization via PEFT)
-   **Reward Model**: `facebook/roberta-hate-speech-dynabench-r4-target`
-   **Core Libraries**:
    -   `PyTorch`
    -   `transformers`
    -   `peft` (for LoRA)
    -   `trl` (for PPO)
    -   `datasets`
    -   `evaluate`

## Methodology

The process detailed in the notebook (`Lab_3_fine_tune_model_to_detoxify_summaries.ipynb`) follows these steps:

1.  **Model Loading**: An existing FLAN-T5 model, already fine-tuned for dialogue summarization using PEFT, is loaded.
2.  **Reward Model Setup**: A sentiment analysis pipeline is created using the RoBERTa hate speech classifier. The logit of the "nothate" class is used as the reward signal.
3.  **Baseline Evaluation**: The initial toxicity of the summarization model is measured on a test dataset to establish a baseline.
4.  **PPO Fine-Tuning**: The `PPOTrainer` from the `trl` library is initialized. It orchestrates the training loop where:
    a. The model generates summaries for prompts from the `dialogsum` dataset.
    b. The reward model scores these `(prompt, summary)` pairs.
    c. The PPO algorithm updates the model's weights (specifically, the LoRA adapters and a new value head) to maximize the expected reward.
5.  **Final Evaluation**: The toxicity of the fine-tuned model is measured and compared against the baseline to determine the effectiveness of the detoxification.

## Results

The experiment provides a clear framework for detoxification using PPO. However, the quantitative results from the notebook's run indicate that the model's average toxicity score **increased by 73.58%** after the PPO training.

-   **Toxicity Before Detox**: Mean = `0.0242`, Std = `0.0324`
-   **Toxicity After Detox**: Mean = `0.0420`, Std = `0.0391`

This unexpected outcome highlights the sensitivity of RL fine-tuning. Factors such as the number of training steps (`max_ppo_steps = 10`), learning rate, or batch size can significantly impact performance. The repository serves as an excellent starting point for exploring these hyperparameters to achieve the desired reduction in toxicity.

## How to Run

1.  **Clone the repository:**
    ```bash
    git clone [https://github.com/your-username/your-repo-name.git](https://github.com/your-username/your-repo-name.git)
    cd your-repo-name
    ```

2.  **Set up the environment**: The notebook is designed to run on an AWS SageMaker instance (`ml.m5.2xlarge` or similar). The required dependencies are installed directly within the notebook.

3.  **Download Model Checkpoint**: The notebook uses `aws s3 cp` to download a pre-trained PEFT checkpoint. Ensure you have AWS CLI configured if running outside of SageMaker.

4.  **Run the Jupyter Notebook**: Launch and execute the cells in `Lab_3_fine_tune_model_to_detoxify_summaries.ipynb`.

> **Note**: The PPO training loop (cell 24) is computationally intensive and can take 20-30 minutes to run.
